{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ll54gJt3hR"
      },
      "source": [
        "#### Code Reference: Python Machine Learning, 3rd Ed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6CoM_jTt3hS"
      },
      "source": [
        "# Modeling Sequential Data Using Recurrent Neural Networks (Part 1/2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHqvUk9kt3hT"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wvo3vxr8t3hW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "rnn_layer = tf.keras.layers.SimpleRNN(\n",
        "    units=2, use_bias=True, \n",
        "    return_sequences=True)\n",
        "rnn_layer.build(input_shape=(None, None, 5))\n",
        "\n",
        "w_xh, w_oo, b_h = rnn_layer.weights\n",
        "\n",
        "print('W_xh 크기:', w_xh.shape)\n",
        "print('W_oo 크기:', w_oo.shape)\n",
        "print('b_h 크기:', b_h.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THMnNQoEt3hW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "x_seq = tf.convert_to_tensor(\n",
        "    [[1.0]*5, [2.0]*5, [3.0]*5],\n",
        "    dtype=tf.float32)\n",
        "\n",
        "\n",
        "## SimepleRNN의 출력:\n",
        "output = rnn_layer(tf.reshape(x_seq, shape=(1, 3, 5)))\n",
        "\n",
        "## 수동으로 출력 계산하기:\n",
        "out_man = []\n",
        "for t in range(len(x_seq)):\n",
        "    xt = tf.reshape(x_seq[t], (1, 5))\n",
        "    print('타임 스텝 {} =>'.format(t))\n",
        "    print('   입력           :', xt.numpy())\n",
        "    \n",
        "    ht = tf.matmul(xt, w_xh) + b_h    \n",
        "    print('   은닉           :', ht.numpy())\n",
        "    \n",
        "    if t>0:\n",
        "        prev_o = out_man[t-1]\n",
        "    else:\n",
        "        prev_o = tf.zeros(shape=(ht.shape))\n",
        "        \n",
        "    ot = ht + tf.matmul(prev_o, w_oo)\n",
        "    ot = tf.math.tanh(ot)\n",
        "    out_man.append(ot)\n",
        "    print('   출력 (수동)     :', ot.numpy())\n",
        "    print('   SimpleRNN 출력 :'.format(t), output[0][t].numpy())\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNOM8cBit3hX"
      },
      "source": [
        "# Implementing RNNs for sequence modeling in TensorFlow\n",
        "## Project one: predicting the sentiment of IMDb movie reviews\n",
        "### Preparing the movie review data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDdVCNv5t3hY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUgiOmnUv7DQ"
      },
      "outputs": [],
      "source": [
        "# 코랩에서 실행하는 경우 다음 코드를 실행하세요.\n",
        "!mkdir ../ch08\n",
        "!wget https://github.com/rickiepark/python-machine-learning-book-3rd-edition/raw/master/ch08/movie_data.csv.gz -O ../ch08/movie_data.csv.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HcKmMPmt3hY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gzip\n",
        "import shutil\n",
        "\n",
        "\n",
        "with gzip.open('../ch08/movie_data.csv.gz', 'rb') as f_in, open('movie_data.csv', 'wb') as f_out:\n",
        "    shutil.copyfileobj(f_in, f_out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-9BuHr_t3hY"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "\n",
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpF_RvlVt3hY"
      },
      "outputs": [],
      "source": [
        "# 단계 1: 데이터셋 만들기\n",
        "target = df.pop('sentiment')\n",
        "\n",
        "ds_raw = tf.data.Dataset.from_tensor_slices(\n",
        "    (df.values, target.values))\n",
        "\n",
        "## 확인:\n",
        "for ex in ds_raw.take(3):\n",
        "    tf.print(ex[0].numpy()[0][:50], ex[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IBmW08tt3hY"
      },
      "source": [
        " * **Train/validaiton/test splits**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89xqs9SXt3hZ"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(1)\n",
        "\n",
        "ds_raw = ds_raw.shuffle(\n",
        "    50000, reshuffle_each_iteration=False)\n",
        "\n",
        "ds_raw_test = ds_raw.take(25000)\n",
        "ds_raw_train_valid = ds_raw.skip(25000)\n",
        "ds_raw_train = ds_raw_train_valid.take(20000)\n",
        "ds_raw_valid = ds_raw_train_valid.skip(20000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eB8MAjOt3hZ"
      },
      "source": [
        " * **Tokenizer and Encoder**\n",
        "   * `tfds.deprecated.text.Tokenizer`: https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/Tokenizer\n",
        "   * `tfds.deprecated.text.TokenTextEncoder`: https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/TokenTextEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzP-YuF3t3hZ"
      },
      "source": [
        " * **Encoding sequences: keeping the last 100 items in each sequence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH3IFCust3hZ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "## 단계 2: 고유 토큰(단어) 찾기\n",
        "from collections import Counter\n",
        "\n",
        "try:\n",
        "    tokenizer = tfds.features.text.Tokenizer()\n",
        "except AttributeError:\n",
        "    tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "token_counts = Counter()\n",
        "\n",
        "for example in ds_raw_train:\n",
        "    tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
        "    token_counts.update(tokens)\n",
        "    \n",
        "print('어휘 사전 크기:', len(token_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzd68Ikut3hZ"
      },
      "outputs": [],
      "source": [
        "## 단계 3: 고유 토큰을 정수로 인코딩하기\n",
        "try:\n",
        "    encoder = tfds.features.text.TokenTextEncoder(token_counts)\n",
        "except AttributeError:\n",
        "    encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
        "\n",
        "example_str = 'This is an example!'\n",
        "encoder.encode(example_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2_YKm8bt3ha"
      },
      "outputs": [],
      "source": [
        "## 단계 3-A: 변환 함수 정의하기\n",
        "def encode(text_tensor, label):\n",
        "    text = text_tensor.numpy()[0]\n",
        "    encoded_text = encoder.encode(text)\n",
        "    return encoded_text, label\n",
        "\n",
        "## 단계 3-B: 인코딩 함수를 텐서플로 연산으로 감싸기\n",
        "def encode_map_fn(text, label):\n",
        "    return tf.py_function(encode, inp=[text, label], \n",
        "                          Tout=(tf.int64, tf.int64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frJTGpBxt3ha"
      },
      "outputs": [],
      "source": [
        "ds_train = ds_raw_train.map(encode_map_fn)\n",
        "ds_valid = ds_raw_valid.map(encode_map_fn)\n",
        "ds_test = ds_raw_test.map(encode_map_fn)\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "for example in ds_train.shuffle(1000).take(5):\n",
        "    print('시퀀스 길이:', example[0].shape)\n",
        "    \n",
        "example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoiIum3jt3ha"
      },
      "source": [
        " * **batch() 대 padded_batch()**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdUNeW8rt3ha"
      },
      "source": [
        "```python\n",
        "\n",
        "# 에러 발생\n",
        "BATCH_SIZE = 32\n",
        "train_data = all_encoded_data.batch(BATCH_SIZE)\n",
        "\n",
        "next(iter(train_data))\n",
        "\n",
        "# 이 코드는 에러를 발생시킵니다\n",
        "# 이 데이터셋에는 .batch()를 적용할 수 없습니다\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXrml4TEt3ha"
      },
      "outputs": [],
      "source": [
        "## 일부 데이터 추출하기\n",
        "ds_subset = ds_train.take(8)\n",
        "for example in ds_subset:\n",
        "    print('개별 샘플 크기:', example[0].shape)\n",
        "\n",
        "## 배치 데이터 만들기\n",
        "ds_batched = ds_subset.padded_batch(\n",
        "    4, padded_shapes=([-1], []))\n",
        "\n",
        "for batch in ds_batched:\n",
        "    print('배치 차원:', batch[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2A9ol3Jt3hb"
      },
      "outputs": [],
      "source": [
        "## 배치 데이터 만들기\n",
        "train_data = ds_train.padded_batch(\n",
        "    32, padded_shapes=([-1],[]))\n",
        "\n",
        "valid_data = ds_valid.padded_batch(\n",
        "    32, padded_shapes=([-1],[]))\n",
        "\n",
        "test_data = ds_test.padded_batch(\n",
        "    32, padded_shapes=([-1],[]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUybBb3zt3hb"
      },
      "source": [
        "### Embedding layers for sentence encoding\n",
        "\n",
        " * `input_dim`: 단어 개수, 즉 최대 정수 인덱스 + 1.\n",
        " * `output_dim`: \n",
        " * `input_length`: (패딩된) 시퀀스 길이\n",
        "    * 예를 들어, `'This is an example' -> [0, 0, 0, 0, 0, 0, 3, 1, 8, 9]`   \n",
        "    => input_lenght는 10\n",
        "\n",
        " * 이 층을 호출할 때 입력으로 정수값을 받습니다. 임베딩 층이 정수를 `[output_dim]` 크기의 실수 벡터로 변환합니다\n",
        "   * 입력 크기가 `[BATCH_SIZE]`이면 출력 크기는 `[BATCH_SIZE, output_dim]`가 됩니다\n",
        "   * 입력 크기가 `[BATCH_SIZE, 10]`이면 출력 크기는 `[BATCH_SIZE, 10, output_dim]`가 됩니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8i2mrfBWt3hb"
      },
      "outputs": [],
      "source": [
        "Image(url='https://git.io/JLdV0', width=700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esPfmEMMt3hb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=100,\n",
        "                    output_dim=6,\n",
        "                    input_length=20,\n",
        "                    name='embed-layer'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqwKaKkqt3hb"
      },
      "source": [
        "### Building an RNN model\n",
        "\n",
        "* **Keras RNN layers:**\n",
        "  * `tf.keras.layers.SimpleRNN(units, return_sequences=False)`\n",
        "  * `tf.keras.layers.LSTM(..)`\n",
        "  * `tf.keras.layers.GRU(..)`\n",
        "  * `tf.keras.layers.Bidirectional()`\n",
        " \n",
        "* **Determine `return_sequenes=?`**\n",
        "  * 다층 RNN이면 마지막 층을 제외하고 모든 RNN 층을 `return_sequenes=True`로 지정합니다\n",
        "  * 마지막 RNN 층은 문제의 종류에 따라 결정됩니다:\n",
        "     * 다대다: -> `return_sequences=True`\n",
        "     * 다대일: -> `return_sequenes=False`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy4iAXCHt3hc"
      },
      "outputs": [],
      "source": [
        "## SimpleRNN 층으로 RNN 모델 만들기\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP_dd6GUt3hc"
      },
      "outputs": [],
      "source": [
        "## LSTM 층으로 RNN 모델 만들기\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UQfBf4yt3hc"
      },
      "outputs": [],
      "source": [
        "## GRU 층으로 RNN 모델 만들기\n",
        "from tensorflow.keras.layers import GRU\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFSW6uDst3hc"
      },
      "source": [
        "### Building an RNN model for the sentiment analysis task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FszX7_Qft3hc"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 20\n",
        "vocab_size = len(token_counts) + 2\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "## 모델 생성\n",
        "bi_lstm_model = \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "bi_lstm_model.summary()\n",
        "\n",
        "## 컴파일과 훈련:\n",
        "bi_lstm_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history = bi_lstm_model.fit(\n",
        "    train_data, \n",
        "    validation_data=valid_data, \n",
        "    epochs=10)\n",
        "\n",
        "## 테스트 데이터에서 평가\n",
        "test_results= bi_lstm_model.evaluate(test_data)\n",
        "print('테스트 정확도: {:.2f}%'.format(test_results[1]*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAYzzBm2t3hd"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('models'):\n",
        "    os.mkdir('models')\n",
        "\n",
        "\n",
        "bi_lstm_model.save('models/Bidir-LSTM-full-length-seq.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOGhajp5t3hd"
      },
      "source": [
        " * **Trying SimpleRNN with short sequences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDbCSxpIt3hd"
      },
      "outputs": [],
      "source": [
        "def preprocess_datasets(\n",
        "    ds_raw_train, \n",
        "    ds_raw_valid, \n",
        "    ds_raw_test,\n",
        "    max_seq_length=None,\n",
        "    batch_size=32):\n",
        "    \n",
        "    ## 단계 1: (데이터셋 만들기 이미 완료)\n",
        "    ## 단계 2: 고유 토큰 찾기\n",
        "    try:\n",
        "        tokenizer = tfds.features.text.Tokenizer()\n",
        "    except AttributeError:\n",
        "        tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "    token_counts = Counter()\n",
        "\n",
        "    for example in ds_raw_train:\n",
        "        tokens = tokenizer.tokenize(example[0].numpy()[0])\n",
        "        if max_seq_length is not None:\n",
        "            tokens = tokens[-max_seq_length:]\n",
        "        token_counts.update(tokens)\n",
        "\n",
        "    print('어휘 사전 크기:', len(token_counts))\n",
        "\n",
        "\n",
        "    ## 단계 3: 텍스트 인코딩하기\n",
        "    try:\n",
        "        encoder = tfds.features.text.TokenTextEncoder(token_counts)\n",
        "    except AttributeError:\n",
        "        encoder = tfds.deprecated.text.TokenTextEncoder(token_counts)\n",
        "\n",
        "    def encode(text_tensor, label):\n",
        "        text = text_tensor.numpy()[0]\n",
        "        encoded_text = encoder.encode(text)\n",
        "        if max_seq_length is not None:\n",
        "            encoded_text = encoded_text[-max_seq_length:]\n",
        "        return encoded_text, label\n",
        "\n",
        "    def encode_map_fn(text, label):\n",
        "        return tf.py_function(encode, inp=[text, label], \n",
        "                              Tout=(tf.int64, tf.int64))\n",
        "\n",
        "    ds_train = ds_raw_train.map(encode_map_fn)\n",
        "    ds_valid = ds_raw_valid.map(encode_map_fn)\n",
        "    ds_test = ds_raw_test.map(encode_map_fn)\n",
        "\n",
        "    ## 단계 4: 배치 데이터 만들기\n",
        "    train_data = ds_train.padded_batch(\n",
        "        batch_size, padded_shapes=([-1],[]))\n",
        "\n",
        "    valid_data = ds_valid.padded_batch(\n",
        "        batch_size, padded_shapes=([-1],[]))\n",
        "\n",
        "    test_data = ds_test.padded_batch(\n",
        "        batch_size, padded_shapes=([-1],[]))\n",
        "\n",
        "    return (train_data, valid_data, \n",
        "            test_data, len(token_counts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVuxsNTRt3hd"
      },
      "outputs": [],
      "source": [
        "def build_rnn_model(embedding_dim, vocab_size,\n",
        "                    recurrent_type='SimpleRNN',\n",
        "                    n_recurrent_units=64,\n",
        "                    n_recurrent_layers=1,\n",
        "                    bidirectional=True):\n",
        "\n",
        "    tf.random.set_seed(1)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    model.add(\n",
        "        Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            name='embed-layer')\n",
        "    )\n",
        "    \n",
        "    for i in range(n_recurrent_layers):\n",
        "        return_sequences = (i < n_recurrent_layers-1)\n",
        "            \n",
        "        if recurrent_type == 'SimpleRNN':\n",
        "            recurrent_layer = SimpleRNN(\n",
        "                units=n_recurrent_units, \n",
        "                return_sequences=return_sequences,\n",
        "                name='simprnn-layer-{}'.format(i))\n",
        "        elif recurrent_type == 'LSTM':\n",
        "            recurrent_layer = LSTM(\n",
        "                units=n_recurrent_units, \n",
        "                return_sequences=return_sequences,\n",
        "                name='lstm-layer-{}'.format(i))\n",
        "        elif recurrent_type == 'GRU':\n",
        "            recurrent_layer = GRU(\n",
        "                units=n_recurrent_units, \n",
        "                return_sequences=return_sequences,\n",
        "                name='gru-layer-{}'.format(i))\n",
        "        \n",
        "        if bidirectional:\n",
        "            recurrent_layer = Bidirectional(\n",
        "                recurrent_layer, name='bidir-'+recurrent_layer.name)\n",
        "            \n",
        "        model.add(recurrent_layer)\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaDu41Pht3hd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Bidirectional\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "embedding_dim = 20\n",
        "max_seq_length = 100\n",
        "\n",
        "train_data, valid_data, test_data, n = preprocess_datasets(\n",
        "    ds_raw_train, ds_raw_valid, ds_raw_test, \n",
        "    max_seq_length=max_seq_length, \n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "\n",
        "vocab_size = n + 2\n",
        "\n",
        "rnn_model = build_rnn_model(\n",
        "    embedding_dim, vocab_size,\n",
        "    recurrent_type='SimpleRNN', \n",
        "    n_recurrent_units=64,\n",
        "    n_recurrent_layers=1,\n",
        "    bidirectional=True)\n",
        "\n",
        "rnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q43zdoSVt3hd"
      },
      "outputs": [],
      "source": [
        "rnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history = rnn_model.fit(\n",
        "    train_data, \n",
        "    validation_data=valid_data, \n",
        "    epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaiatV_It3he"
      },
      "outputs": [],
      "source": [
        "results = rnn_model.evaluate(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6QyEwQSt3he"
      },
      "outputs": [],
      "source": [
        "print('테스트 정확도: {:.2f}%'.format(results[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjPp6Koit3he"
      },
      "source": [
        "## Optional exercise: \n",
        "\n",
        "### Uni-directional SimpleRNN with full-length sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDMMjV1xt3he"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "embedding_dim = 20\n",
        "max_seq_length = None\n",
        "\n",
        "train_data, valid_data, test_data, n = preprocess_datasets(\n",
        "    ds_raw_train, ds_raw_valid, ds_raw_test, \n",
        "    max_seq_length=max_seq_length, \n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "\n",
        "vocab_size = n + 2\n",
        "\n",
        "rnn_model = build_rnn_model(\n",
        "    embedding_dim, vocab_size,\n",
        "    recurrent_type='SimpleRNN', \n",
        "    n_recurrent_units=64,\n",
        "    n_recurrent_layers=1,\n",
        "    bidirectional=False)\n",
        "\n",
        "rnn_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR8OJjstt3he"
      },
      "outputs": [],
      "source": [
        "rnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "history = rnn_model.fit(\n",
        "    train_data, \n",
        "    validation_data=valid_data, \n",
        "    epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P9l6HEHt3he"
      },
      "source": [
        "# Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAw6wJ0jt3hf"
      },
      "source": [
        "### A -- An alternative way to get the dataset: using tensorflow_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm4eEIM7t3hf"
      },
      "outputs": [],
      "source": [
        "imdb_bldr = tfds.builder('imdb_reviews')\n",
        "print(imdb_bldr.info)\n",
        "\n",
        "imdb_bldr.download_and_prepare()\n",
        "\n",
        "datasets = imdb_bldr.as_dataset(shuffle_files=False)\n",
        "\n",
        "datasets.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHn1LXgkt3hf"
      },
      "outputs": [],
      "source": [
        "imdb_train = datasets['train']\n",
        "imdb_train = datasets['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2THlQCVWt3hf"
      },
      "source": [
        "### B -- Tokenizer and Encoder\n",
        "\n",
        " * `tfds.deprecated.text.Tokenizer`: https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/Tokenizer\n",
        " * `tfds.deprecated.text.TokenTextEncoder`: https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/TokenTextEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZrY4Fcnt3hf"
      },
      "outputs": [],
      "source": [
        "vocab_set = {'a', 'b', 'c', 'd'}\n",
        "encoder = tfds.deprecated.text.TokenTextEncoder(vocab_set)\n",
        "print(encoder)\n",
        "\n",
        "print(encoder.encode(b'a b c d, , : .'))\n",
        "\n",
        "print(encoder.encode(b'a b c d e f g h i z'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY6z1aTit3hg"
      },
      "source": [
        "### C -- Text Pre-processing with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRO2xhDKt3hg"
      },
      "outputs": [],
      "source": [
        "TOP_K = 200\n",
        "MAX_LEN = 10\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=TOP_K)\n",
        "\n",
        "tokenizer.fit_on_texts(['this is an example', 'je suis en forme '])\n",
        "sequences = tokenizer.texts_to_sequences(['this is an example', 'je suis en forme '])\n",
        "print(sequences)\n",
        "\n",
        "tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vePZ3UTot3hg",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "TOP_K = 20000\n",
        "MAX_LEN = 500\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=TOP_K)\n",
        "\n",
        "tokenizer.fit_on_texts(\n",
        "    [example['text'].numpy().decode('utf-8') \n",
        "     for example in imdb_train])\n",
        "\n",
        "x_train = tokenizer.texts_to_sequences(\n",
        "    [example['text'].numpy().decode('utf-8')\n",
        "     for example in imdb_train])\n",
        "\n",
        "print(len(x_train))\n",
        "\n",
        "\n",
        "x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    x_train, maxlen=MAX_LEN)\n",
        "\n",
        "print(x_train_padded.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FYRIc-St3hg"
      },
      "source": [
        "### D -- Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEuT0TEut3hh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "embed = Embedding(input_dim=100, output_dim=4)\n",
        "\n",
        "inp_arr = np.array([1, 98, 5, 6, 67, 45])\n",
        "tf.print(embed(inp_arr))\n",
        "tf.print(embed(inp_arr).shape)\n",
        "\n",
        "tf.print(embed(np.array([1])))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}